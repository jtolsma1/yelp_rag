{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8ab32924",
   "metadata": {},
   "source": [
    "# Relevant Text Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c9f54414",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "current_dir = \"/Users/josephtolsma/Documents/dev/yelp_rag\"\n",
    "sys.path.insert(0,current_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f288207e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import faiss\n",
    "from src.config import DATA_DIR_PROC,INDEX_DIR,TOPICS,EMBEDDING_MODEL_NAME\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "864030f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_colwidth\",None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fa51818",
   "metadata": {},
   "source": [
    "**Pick a sample review id to use for the initial function build**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "72ec3fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_df = pd.read_parquet(os.path.join(DATA_DIR_PROC,\"review_chunks.parquet\"))\n",
    "rid = chunk_df[\"business_id\"].sample(random_state=42).values[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64cabb4e",
   "metadata": {},
   "source": [
    "**Build retrieval functionality**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a2e6c7c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embedding_model(model_name,device = \"mps\") -> SentenceTransformer:\n",
    "    \"\"\"\n",
    "    Load a local embedding model using sentence_transformer library\n",
    "    \"\"\"\n",
    "    return SentenceTransformer(model_name,device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "513d45d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_similarity_arrays_to_df(score_array,index_array,metadata,metadata_cols):\n",
    "    \"\"\"\n",
    "    Converts raw similarity arrays from FAISS to a dataframe of relevant text chunks.\n",
    "    Limits returned values to one chunk per business to avoid duplicated statements.\n",
    "    @param score_array: similarity scores returned by searching query verbatim in the corpus of reviews\n",
    "    @param index_array: indicies associated with each chunk returned by the FAISS query search\n",
    "    @param metadata: metadata dataframe associated with each index, such as business id and (non-encoded) text chunk\n",
    "    @param metadata_cols: columns in the metadata dataframe relevant to the final result\n",
    "    @returns dataframe of most similar text chunks and identifiers for each chunk\n",
    "    \"\"\"\n",
    "\n",
    "    chunk_id_col = metadata_cols[\"chunk_id_col\"]\n",
    "    business_id_col = metadata_cols[\"business_id_col\"]\n",
    "    chunk_col = metadata_cols[\"chunk_col\"]\n",
    "\n",
    "    df = pd.DataFrame({\n",
    "    \"scores\":score_array[0],\n",
    "    \"indicies\":index_array[0]\n",
    "    }).assign(\n",
    "        chunk_id = lambda df: metadata.iloc[df[\"indicies\"]][chunk_id_col].values,\n",
    "        business_id = lambda df: metadata.loc[df[\"indicies\"]][business_id_col].values,\n",
    "        chunk = lambda df: metadata.loc[df[\"indicies\"]][chunk_col].values\n",
    "    )\n",
    "\n",
    "    df = df.sort_values(by = \"scores\",ascending=False)\n",
    "    # note to future self: right now, the notebook is pulling in only one restaurant file so dropping duplicates will shrink this dataframe to one row always\n",
    "    df = df.drop_duplicates(subset = business_id_col,keep = \"first\")\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "afb52f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = faiss.read_index(os.path.join(INDEX_DIR,f\"{rid}.faiss\"))\n",
    "meta = pd.read_parquet(os.path.join(INDEX_DIR,f\"{rid}_meta.parquet\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6cda1c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_embedding_model(EMBEDDING_MODEL_NAME,\"mps\")\n",
    "query = model.encode([\"I like chicken, I like liver, meow mix please deliver\"],convert_to_numpy=True,normalize_embeddings=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e73660ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>scores</th>\n",
       "      <th>indicies</th>\n",
       "      <th>chunk_id</th>\n",
       "      <th>business_id</th>\n",
       "      <th>chunk</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.480943</td>\n",
       "      <td>116</td>\n",
       "      <td>Y9VMWfb3lmIh_s80uJ2P1A_0</td>\n",
       "      <td>MG_wIwRBwyNnCAEMXe9Jqw</td>\n",
       "      <td>Went here from all the reviews! Best chicken I've had. I like the corn meal batter. The butter and maple syrup was deliciously amazing. The waffle now that could've been cooked longer it was still gooey however the chicken takes the cake here!</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     scores  indicies                  chunk_id             business_id  \\\n",
       "0  0.480943       116  Y9VMWfb3lmIh_s80uJ2P1A_0  MG_wIwRBwyNnCAEMXe9Jqw   \n",
       "\n",
       "                                                                                                                                                                                                                                                 chunk  \n",
       "0  Went here from all the reviews! Best chicken I've had. I like the corn meal batter. The butter and maple syrup was deliciously amazing. The waffle now that could've been cooked longer it was still gooey however the chicken takes the cake here!  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "D,I = index.search(query,k = 10)\n",
    "metadata_cols = {\n",
    "    \"chunk_id_col\":\"chunk_id\",\n",
    "    \"business_id_col\":\"business_id\",\n",
    "    \"chunk_col\":\"chunk\"\n",
    "}\n",
    "convert_similarity_arrays_to_df(D,I,meta,metadata_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b9b3743",
   "metadata": {},
   "source": [
    "**Loop Retrieval Functionality Through All Topics & Restaurants**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b070eede",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Data must be 1-dimensional, got ndarray of shape (1, 25) instead",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      2\u001b[39m query = model.encode([keywords],convert_to_numpy=\u001b[38;5;28;01mTrue\u001b[39;00m,normalize_embeddings=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m      3\u001b[39m D,I = index.search(query,k = \u001b[32m25\u001b[39m)\n\u001b[32m      4\u001b[39m meta = pd.concat(\n\u001b[32m      5\u001b[39m     [meta,\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m     \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mSeries\u001b[49m\u001b[43m(\u001b[49m\u001b[43mD\u001b[49m\u001b[43m)\u001b[49m],\n\u001b[32m      7\u001b[39m     axis = \u001b[32m1\u001b[39m\n\u001b[32m      8\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/dev/yelp_rag/.venv/lib/python3.12/site-packages/pandas/core/series.py:587\u001b[39m, in \u001b[36mSeries.__init__\u001b[39m\u001b[34m(self, data, index, dtype, name, copy, fastpath)\u001b[39m\n\u001b[32m    585\u001b[39m         data = data.copy()\n\u001b[32m    586\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m587\u001b[39m     data = \u001b[43msanitize_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    589\u001b[39m     manager = _get_option(\u001b[33m\"\u001b[39m\u001b[33mmode.data_manager\u001b[39m\u001b[33m\"\u001b[39m, silent=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m    590\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m manager == \u001b[33m\"\u001b[39m\u001b[33mblock\u001b[39m\u001b[33m\"\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/dev/yelp_rag/.venv/lib/python3.12/site-packages/pandas/core/construction.py:656\u001b[39m, in \u001b[36msanitize_array\u001b[39m\u001b[34m(data, index, dtype, copy, allow_2d)\u001b[39m\n\u001b[32m    653\u001b[39m             subarr = cast(np.ndarray, subarr)\n\u001b[32m    654\u001b[39m             subarr = maybe_infer_to_datetimelike(subarr)\n\u001b[32m--> \u001b[39m\u001b[32m656\u001b[39m subarr = \u001b[43m_sanitize_ndim\u001b[49m\u001b[43m(\u001b[49m\u001b[43msubarr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_2d\u001b[49m\u001b[43m=\u001b[49m\u001b[43mallow_2d\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    658\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(subarr, np.ndarray):\n\u001b[32m    659\u001b[39m     \u001b[38;5;66;03m# at this point we should have dtype be None or subarr.dtype == dtype\u001b[39;00m\n\u001b[32m    660\u001b[39m     dtype = cast(np.dtype, dtype)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/dev/yelp_rag/.venv/lib/python3.12/site-packages/pandas/core/construction.py:715\u001b[39m, in \u001b[36m_sanitize_ndim\u001b[39m\u001b[34m(result, data, dtype, index, allow_2d)\u001b[39m\n\u001b[32m    713\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m allow_2d:\n\u001b[32m    714\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m result\n\u001b[32m--> \u001b[39m\u001b[32m715\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    716\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mData must be 1-dimensional, got ndarray of shape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdata.shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m instead\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    717\u001b[39m     )\n\u001b[32m    718\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_object_dtype(dtype) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(dtype, ExtensionDtype):\n\u001b[32m    719\u001b[39m     \u001b[38;5;66;03m# i.e. NumpyEADtype(\"O\")\u001b[39;00m\n\u001b[32m    721\u001b[39m     result = com.asarray_tuplesafe(data, dtype=np.dtype(\u001b[33m\"\u001b[39m\u001b[33mobject\u001b[39m\u001b[33m\"\u001b[39m))\n",
      "\u001b[31mValueError\u001b[39m: Data must be 1-dimensional, got ndarray of shape (1, 25) instead"
     ]
    }
   ],
   "source": [
    "for topic,keywords in TOPICS.items():\n",
    "    query = model.encode([keywords],convert_to_numpy=True,normalize_embeddings=True)\n",
    "    D,I = index.search(query,k = 25)\n",
    "    meta = pd.concat(\n",
    "        [meta,\n",
    "        pd.Series(D)],\n",
    "        axis = 1\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cc48d04",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10f37a98",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb0e4609",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53c5e42c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
